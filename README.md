# GaneshaBot üïâÔ∏è

GaneshaBot is a voice and text-powered conversational AI designed to embody the persona of Lord Ganesha. It leverages a modern tech stack to provide an interactive experience where users can ask questions and receive answers grounded in Ganesha's lore and scriptures.

---

## ‚ú® Features

* **Voice and Text Interaction:** Communicate with the bot through either spoken words or typed messages.
* **Ganesha Persona:** The bot's responses are generated by Llama 3, instructed to adopt the wise and benevolent persona of Ganesha.
* **Knowledge-Grounded Responses:** Utilizes Retrieval-Augmented Generation (RAG) to ensure answers are based on a dedicated knowledge base of Ganesha's lores.
* **Local First:** All AI models (STT, LLM, TTS) run locally for privacy and performance.

---

## üèóÔ∏è System Architecture

The GaneshaBot employs a client-server architecture. The frontend is a web application built with **TypeScript**, and the backend is a **Python** server using the Flask framework.

   TypeScript    |----->|    Python Backend    |----->|      AI Models      |
    Frontend     |<-----|     (Flask API)      |<-----| (Whisper, Llama 3,  |

(User Interaction: Voice & Text)


* **Frontend (Client):** The user interacts with the bot through a web interface. It captures audio input, sends it to the backend, plays back the audio response, and handles text-based conversations.

* **Backend (Server):** The Flask application exposes API endpoints to handle requests. It orchestrates the entire process of transcription, language model interaction, and text-to-speech conversion.

* **AI Models:** The backend leverages several open-source AI models:
    * **Whisper.cpp:** For speech-to-text transcription.
    * **GPT-4All (Llama 3 8B Instruct):** For generating conversational responses.
    * **RAG (Retrieval-Augmented Generation):** To provide the LLM with context from Ganesha's lore.
    * **Piper:** For text-to-speech synthesis.

---

## üõ†Ô∏è Tech Stack & Models

### Frontend
* **TypeScript:** A statically typed superset of JavaScript that enhances code quality and maintainability.

### Backend
* **Python:** The primary language for the backend logic.
* **Flask:** A lightweight web framework for building the API.
* **Flask-CORS:** A Flask extension for handling Cross-Origin Resource Sharing (CORS).
* **ffmpeg:** A command-line tool for converting audio files into the required format for Whisper.cpp.

### AI/ML Models
* **Whisper.cpp:**
    * **Purpose:** High-performance speech-to-text transcription.
    * **Model:** `ggml-medium.bin` - A quantized version of OpenAI's Whisper model, optimized for CPU execution.
* **GPT-4All (Llama 3 8B Instruct):**
    * **Purpose:** The core language model that generates intelligent and context-aware responses. It's instructed to respond in the persona of Ganesha.
* **Retrieval-Augmented Generation (RAG):**
    * **Purpose:** To ground the LLM's responses in factual information from a specific knowledge base (Ganesha's lores and scriptures). This is achieved by creating embeddings of the lore and retrieving relevant passages to be included in the prompt to the LLM.
* **Piper:**
    * **Purpose:** A fast, local neural text-to-speech system that converts the LLM's text response into natural-sounding speech.

---

## üîå API Documentation

The backend exposes the following APIs for the frontend.

### `POST /transcribe`
Handles voice input from the user.

* **Request:** `multipart/form-data` with a single field:
    * `audio`: The audio file (e.g., in `.webm` format) recorded by the user.

* **Response:** `application/json`
    ```json
    {
      "id": "unique_file_id",
      "transcription": "The transcribed text from the audio.",
      "ganesha_response": {
        "lang": "en",
        "blessing_open": "...",
        "answer": "The response from the Llama 3 model.",
        "blessing_close": "...",
        "refusal": false,
        "refusal_reason": null
      },
      "audio_url": "http://localhost:5000/audio/output.wav"
    }
    ```

### `POST /text-message`
Handles text input from the user.

* **Request:** `application/json`
    ```json
    {
      "message": "The user's text message."
    }
    ```

* **Response:** `application/json`
    ```json
    {
      "id": "unique_file_id",
      "transcription": "The original user message.",
      "ganesha_response": {
        "lang": "en",
        "blessing_open": "...",
        "answer": "The response from the Llama 3 model.",
        "blessing_close": "...",
        "refusal": false,
        "refusal_reason": null
      }
    }
    ```

### `GET /audio/<filename>`
Serves the generated audio file to the frontend.

* **URL Parameter:** `filename` (e.g., `output.wav`)
* **Response:** The audio file.

---

## ‚öôÔ∏è Workflow

### Voice Interaction Workflow
1.  **Audio Recording (Frontend):** The user records a message using the microphone in the web application.
2.  **API Request (Frontend):** The recorded audio is sent as a `POST` request to the `/transcribe` endpoint.
3.  **Audio Conversion (Backend):** The backend receives the `.webm` audio file and uses `ffmpeg` to convert it to a 16-bit WAV file with a 16kHz sample rate, the required format for Whisper.cpp.
4.  **Transcription (Backend):** The converted WAV file is passed to the `whisper-cli` command-line tool, which transcribes the audio to text.
5.  **LLM Processing (Backend):** The transcribed text is used as a query for the RAG system to find relevant lore. The text and the retrieved context are passed to the Llama 3 model, which generates a response in the persona of Ganesha.
6.  **Text-to-Speech (Backend):** The text response from the LLM is passed to the Piper TTS engine, which generates an audio file (`output.wav`).
7.  **API Response (Backend):** The backend sends a JSON response to the frontend containing the transcription, the text response from Ganesha, and the URL to the generated audio file.
8.  **Audio Playback (Frontend):** The frontend receives the response and uses the `audio_url` to fetch and play the audio response to the user.

### Text Interaction Workflow
1.  **Text Input (Frontend):** The user types a message in the chat interface.
2.  **API Request (Frontend):** The text message is sent as a JSON payload in a `POST` request to the `/text-message` endpoint.
3.  **LLM Processing (Backend):** The text is used as a query for the RAG system. The text and the retrieved context are passed to the Llama 3 model, which generates a text-based response.
4.  **API Response (Backend):** The backend sends a JSON response to the frontend containing the original message and the generated response from Ganesha.
5.  **Display Response (Frontend):** The frontend displays the text response in the chat interface.



